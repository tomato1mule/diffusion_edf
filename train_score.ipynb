{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_JIT_USE_NNC_NOT_NVFUSER\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple, Optional, Union, Iterable\n",
    "import datetime\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose\n",
    "from e3nn import o3\n",
    "from open3d.visualization.tensorboard_plugin import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from diffusion_edf.embedding import NodeEmbeddingNetwork\n",
    "from diffusion_edf.data import SE3, PointCloud, TargetPoseDemo, DemoSequence, DemoSeqDataset, load_demos, save_demos\n",
    "from diffusion_edf.preprocess import Rescale, NormalizeColor, Downsample, PointJitter, ColorJitter\n",
    "from diffusion_edf.wigner import TransformFeatureQuaternion\n",
    "from diffusion_edf.score_model import ScoreModel\n",
    "from diffusion_edf import transforms\n",
    "from diffusion_edf.loss import SE3DenoisingDiffusion\n",
    "from diffusion_edf.utils import sample_reference_points\n",
    "from diffusion_edf.dist import diffuse_isotropic_se3, adjoint_inv_tr_isotropic_se3_score, diffuse_isotropic_se3_batched\n",
    "\n",
    "\n",
    "\n",
    "torch.set_printoptions(precision=4, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_len = 0.01\n",
    "scene_voxel_size = 0.01\n",
    "grasp_voxel_size = 0.01\n",
    "\n",
    "scene_voxel_size = scene_voxel_size / unit_len\n",
    "grasp_voxel_size = grasp_voxel_size / unit_len\n",
    "\n",
    "\n",
    "rescale_fn = Rescale(rescale_factor=1/unit_len)\n",
    "recover_scale_fn = Rescale(rescale_factor=unit_len)\n",
    "normalize_color_fn = NormalizeColor(color_mean = torch.tensor([0.5, 0.5, 0.5]), color_std = torch.tensor([0.5, 0.5, 0.5]))\n",
    "recover_color_fn = NormalizeColor(color_mean = -normalize_color_fn.color_mean / normalize_color_fn.color_std, color_std = 1 / normalize_color_fn.color_std)\n",
    "\n",
    "\n",
    "scene_proc_fn = Compose([rescale_fn,\n",
    "                         Downsample(voxel_size=scene_voxel_size, coord_reduction=\"average\"),\n",
    "                         normalize_color_fn])\n",
    "scene_unproc_fn = Compose([recover_color_fn, recover_scale_fn])\n",
    "grasp_proc_fn = Compose([rescale_fn,\n",
    "                         Downsample(voxel_size=grasp_voxel_size, coord_reduction=\"average\"),\n",
    "                         normalize_color_fn])\n",
    "grasp_unproc_fn = Compose([recover_color_fn, recover_scale_fn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "device = 'cuda:0'\n",
    "compile = False\n",
    "\n",
    "irreps_input = o3.Irreps('3x0e')\n",
    "irreps_node_embedding = o3.Irreps('32x0e+16x1e+8x2e') #o3.Irreps('128x0e+64x1e+32x2e')\n",
    "irreps_sh = o3.Irreps('1x0e+1x1e+1x2e')\n",
    "fc_neurons = [128, 64, 64]\n",
    "num_heads = 4\n",
    "alpha_drop = 0.2\n",
    "proj_drop = 0.0\n",
    "drop_path_rate = 0.0\n",
    "irreps_mlp_mid = 2\n",
    "n_scales = 4\n",
    "pool_ratio = 0.5\n",
    "lin_mult = 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hw/anaconda3/envs/diff_edf/lib/python3.8/site-packages/torch/jit/_check.py:181: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\"The TorchScript type system doesn't support \"\n"
     ]
    }
   ],
   "source": [
    "score_model = ScoreModel(irreps_input = irreps_input,\n",
    "                         irreps_emb_init = irreps_node_embedding,\n",
    "                         irreps_sh = irreps_sh,\n",
    "                         fc_neurons_init = [32, 16, 16],\n",
    "                         num_heads = 4,\n",
    "                         n_scales = 4,\n",
    "                         pool_ratio = 0.25,\n",
    "                         dim_mult = [1, 1, 2, 2],\n",
    "                         n_layers = 2,\n",
    "                         gnn_radius = 3.0,\n",
    "                         cutoff_radius = 5.0,\n",
    "                         weight_feature_dim = 20,\n",
    "                         query_downsample_ratio = 0.7,\n",
    "                         device=device,\n",
    "                         lin_mult=lin_mult,\n",
    "                         deterministic = False,\n",
    "                         compile_head = compile)\n",
    "\n",
    "score_model = score_model.to(device)\n",
    "optimizer = torch.optim.Adam(list(score_model.parameters()), lr=3e-4, betas=(0.9, 0.98), eps=1e-09, weight_decay=1e-4, amsgrad=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_log_dir: Optional[str] = None\n",
    "#resume_log_dir: Optional[str] = 'runs/2023_04_21_00-34-42'\n",
    "resume_checkpoint_dir: Optional[str] = None\n",
    "if resume_log_dir is not None:\n",
    "    if resume_checkpoint_dir is None:\n",
    "        resume_checkpoint_dir = sorted(os.listdir(os.path.join(resume_log_dir, f'checkpoint')), key= lambda f:int(f.rstrip('.pt')))[-1]\n",
    "    resume_training = True\n",
    "    if input(f\"Enter 'y' if you want to resume training from checkpoint: {os.path.join(resume_log_dir, f'checkpoint', resume_checkpoint_dir)}\") == 'y':\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError()\n",
    "else:\n",
    "    resume_training = False\n",
    "    resume_log_dir = os.path.join('runs', f\"{datetime.datetime.now().strftime('%Y_%m_%d_%H-%M-%S')}\")\n",
    "\n",
    "writer = SummaryWriter(log_dir=resume_log_dir)\n",
    "log_dir = writer.log_dir\n",
    "\n",
    "if not os.path.exists(os.path.join(log_dir, f'checkpoint')):\n",
    "    os.mkdir(os.path.join(log_dir, f'checkpoint'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = DemoSeqDataset(dataset_dir=\"demo/test_demo\", annotation_file=\"data.yaml\", device=device)\n",
    "train_dataloader = DataLoader(trainset, shuffle=True, collate_fn=lambda x:x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if resume_training:\n",
    "    checkpoint = torch.load(os.path.join(log_dir, f'checkpoint', resume_checkpoint_dir))\n",
    "    score_model.load_state_dict(checkpoint['score_model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    steps = checkpoint['steps']\n",
    "    print(f\"resume training from checkpoint: {os.path.join(log_dir, f'checkpoint', resume_checkpoint_dir)}\")\n",
    "    epoch = epoch + 1\n",
    "else:\n",
    "    epoch = 0\n",
    "    steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 10000\n",
    "N_samples = 10\n",
    "n_epochs_per_checkpoint = 100\n",
    "\n",
    "n_samples_x_ref = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch: 0) Successfully saved logs to: runs/2023_04_22_00-08-58\n",
      "(Epoch: 50) Successfully saved logs to: runs/2023_04_22_00-08-58\n",
      "(Epoch: 100) Successfully saved logs to: runs/2023_04_22_00-08-58\n",
      "(Epoch: 150) Successfully saved logs to: runs/2023_04_22_00-08-58\n",
      "(Epoch: 200) Successfully saved logs to: runs/2023_04_22_00-08-58\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch, max_epochs+1):\n",
    "    for train_batch in train_dataloader:\n",
    "        assert len(train_batch) == 1, \"Batch training is not supported yet.\"\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "\n",
    "        demo_seq: DemoSequence = train_batch[0]\n",
    "        demo: TargetPoseDemo = demo_seq[1]\n",
    "        scene_raw: PointCloud = demo.scene_pc\n",
    "        grasp_raw: PointCloud = demo.grasp_pc\n",
    "        target_poses_raw: SE3 = demo.target_poses\n",
    "        scene_proc: PointCloud = scene_proc_fn(scene_raw).to(device)\n",
    "        grasp_proc: PointCloud = grasp_proc_fn(grasp_raw).to(device)\n",
    "        target_poses: SE3 = rescale_fn(target_poses_raw).to(device)\n",
    "        T_target: torch.Tensor = target_poses.poses\n",
    "\n",
    "\n",
    "        min_time = 1e-3\n",
    "        max_time = 0.2\n",
    "        time_in = (min_time/max_time + torch.rand(1, dtype=T_target.dtype, device=T_target.device) * (1-min_time/max_time))*max_time\n",
    "        eps = time_in / 2\n",
    "        std = torch.sqrt(time_in) * lin_mult\n",
    "        x_ref, n_neighbors = sample_reference_points(PointCloud.transform_pcd(scene_proc, target_poses.inv())[0].points, grasp_proc.points, r=3, n_samples=n_samples_x_ref)\n",
    "        # T, delta_T, (gt_ang_score, gt_lin_score), (gt_ang_score_ref, gt_lin_score_ref) = diffuse_isotropic_se3(T0 = T_target, eps=eps, std=std, x_ref=x_ref, double_precision=True)\n",
    "        T, delta_T, (gt_ang_score, gt_lin_score), (gt_ang_score_ref, gt_lin_score_ref) = diffuse_isotropic_se3_batched(T0 = T_target, eps=eps, std=std, x_ref=x_ref, double_precision=True)\n",
    "        T, delta_T, (gt_ang_score, gt_lin_score), (gt_ang_score_ref, gt_lin_score_ref) = T.squeeze(-2), delta_T.squeeze(-2), (gt_ang_score.squeeze(-2), gt_lin_score.squeeze(-2)), (gt_ang_score_ref.squeeze(-2), gt_lin_score_ref.squeeze(-2))\n",
    "\n",
    "\n",
    "        key_feature = scene_proc.colors\n",
    "        key_coord = scene_proc.points\n",
    "        key_batch = torch.zeros(len(key_coord), device=device, dtype=torch.long)\n",
    "        query_feature = grasp_proc.colors\n",
    "        query_coord = grasp_proc.points\n",
    "        query_batch = torch.zeros(len(query_coord), device=device, dtype=torch.long)\n",
    "        (ang_score, lin_score), query, query_info, key_info = score_model(T=T,\n",
    "                                                                          key_feature=key_feature, key_coord=key_coord, key_batch=key_batch,\n",
    "                                                                          query_feature=query_feature, query_coord=query_coord, query_batch=query_batch,\n",
    "                                                                          info_mode='NONE', time=time_in)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            ang_score_ref, lin_score_ref = adjoint_inv_tr_isotropic_se3_score(x_ref=-x_ref, ang_score=ang_score, lin_score=lin_score)\n",
    "        target_ang_score = gt_ang_score * torch.sqrt(time_in)\n",
    "        target_lin_score = gt_lin_score * torch.sqrt(time_in)\n",
    "        target_ang_score_ref = gt_ang_score_ref * torch.sqrt(time_in)\n",
    "        target_lin_score_ref = gt_lin_score_ref * torch.sqrt(time_in)\n",
    "\n",
    "\n",
    "        ang_score_diff = target_ang_score - ang_score\n",
    "        lin_score_diff = target_lin_score - lin_score\n",
    "        # ang_loss = torch.norm(ang_score_diff, dim=-1).mean(dim=-1)\n",
    "        # lin_loss = torch.norm(lin_score_diff * lin_mult, dim=-1).mean(dim=-1)\n",
    "        ang_loss = torch.sum(torch.square(ang_score_diff), dim=-1).mean(dim=-1)\n",
    "        lin_loss = torch.sum(torch.square(lin_score_diff * lin_mult), dim=-1).mean(dim=-1)\n",
    "        loss = ang_loss + lin_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            writer.add_scalar(tag=\"Loss/train\", scalar_value=loss.item(), global_step=steps)\n",
    "            writer.add_scalar(tag=\"Loss/angular\", scalar_value=ang_loss.item(), global_step=steps)\n",
    "            writer.add_scalar(tag=\"Loss/linear\", scalar_value=lin_loss.item(), global_step=steps)\n",
    "\n",
    "            target_norm_ang, target_norm_lin = torch.norm(target_ang_score.detach(), dim=-1), torch.norm(target_lin_score.detach(), dim=-1) # Shape: (Nbatch, ), (Nbatch, )\n",
    "            score_norm_ang, score_norm_lin = torch.norm(ang_score.detach(), dim=-1), torch.norm(lin_score.detach(), dim=-1)         # Shape: (Nbatch, ), (Nbatch, )\n",
    "            writer.add_scalar(tag=\"norm/target_ang\", scalar_value=target_norm_ang.mean(dim=-1).item(), global_step=steps)\n",
    "            writer.add_scalar(tag=\"norm/target_lin\", scalar_value=target_norm_lin.mean(dim=-1).item(), global_step=steps)\n",
    "            writer.add_scalar(tag=\"norm/inferred_ang\", scalar_value=score_norm_ang.mean(dim=-1).item(), global_step=steps)\n",
    "            writer.add_scalar(tag=\"norm/inferred_lin\", scalar_value=score_norm_lin.mean(dim=-1).item(), global_step=steps)\n",
    "\n",
    "            target_norm_ang_ref, target_norm_lin_ref = torch.norm(target_ang_score_ref.detach(), dim=-1), torch.norm(target_lin_score_ref.detach(), dim=-1) # Shape: (Nbatch, ), (Nbatch, )\n",
    "            score_norm_ang_ref, score_norm_lin_ref = torch.norm(ang_score_ref.detach(), dim=-1), torch.norm(lin_score_ref.detach(), dim=-1)         # Shape: (Nbatch, ), (Nbatch, )\n",
    "            writer.add_scalar(tag=\"norm_ref/target_ang\", scalar_value=target_norm_ang_ref.mean(dim=-1).item(), global_step=steps)\n",
    "            writer.add_scalar(tag=\"norm_ref/target_lin\", scalar_value=target_norm_lin_ref.mean(dim=-1).item(), global_step=steps)\n",
    "            writer.add_scalar(tag=\"norm_ref/inferred_ang\", scalar_value=score_norm_ang_ref.mean(dim=-1).item(), global_step=steps)\n",
    "            writer.add_scalar(tag=\"norm_ref/inferred_lin\", scalar_value=score_norm_lin_ref.mean(dim=-1).item(), global_step=steps)\n",
    "\n",
    "            dp_align_ang = torch.einsum('...i,...i->...', ang_score.detach(), target_ang_score.detach()) # Shape: (Nbatch, )\n",
    "            dp_align_lin = torch.einsum('...i,...i->...', lin_score.detach(), target_lin_score.detach()) # Shape: (Nbatch, )\n",
    "            dp_align_ang_normalized = dp_align_ang / target_norm_ang / score_norm_ang # Shape: (Nbatch, )\n",
    "            dp_align_lin_normalized = dp_align_lin / target_norm_lin / score_norm_lin # Shape: (Nbatch, )\n",
    "            writer.add_scalar(tag=\"alignment/unnormalized/ang\", scalar_value=dp_align_ang.mean(dim=-1).item(), global_step=steps)\n",
    "            writer.add_scalar(tag=\"alignment/unnormalized/lin\", scalar_value=dp_align_lin.mean(dim=-1).item(), global_step=steps)\n",
    "            writer.add_scalar(tag=\"alignment/normalized/ang\", scalar_value=dp_align_ang_normalized.mean(dim=-1).item(), global_step=steps)\n",
    "            writer.add_scalar(tag=\"alignment/normalized/lin\", scalar_value=dp_align_lin_normalized.mean(dim=-1).item(), global_step=steps)\n",
    "\n",
    "        steps += 1\n",
    "\n",
    "    if epoch % n_epochs_per_checkpoint == 0:\n",
    "        torch.save({'epoch': epoch,\n",
    "                    'steps': steps,\n",
    "                    'score_model_state_dict': score_model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    }, os.path.join(log_dir, f'checkpoint/{epoch}.pt'))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            target_pose_pcd = PointCloud.merge(scene_raw, grasp_raw.transformed(target_poses_raw)[0])\n",
    "            diffused_pose_pcd = PointCloud.merge(scene_raw, grasp_raw.transformed( recover_scale_fn(SE3(T)) )[0])\n",
    "            grasp_pcd = grasp_raw\n",
    "\n",
    "            writer.add_3d(\n",
    "                tag = \"Target Pose\",\n",
    "                data = {\n",
    "                    \"vertex_positions\": target_pose_pcd.points.cpu(),\n",
    "                    \"vertex_colors\": target_pose_pcd.colors.cpu(),  # (N, 3)\n",
    "                },\n",
    "                step=epoch,\n",
    "            )\n",
    "\n",
    "            writer.add_3d(\n",
    "                tag = \"Diffused Pose\",\n",
    "                data = {\n",
    "                    \"vertex_positions\": diffused_pose_pcd.points.cpu(),\n",
    "                    \"vertex_colors\": diffused_pose_pcd.colors.cpu(),  # (N, 3)\n",
    "                },\n",
    "                step=epoch,\n",
    "                description=f\"Diffuse time: {time_in.item()} || eps: {eps.item()} || std: {std.item()}\",\n",
    "            )\n",
    "\n",
    "            writer.add_3d(\n",
    "                tag = \"Grasp\",\n",
    "                data = {\n",
    "                    \"vertex_positions\": grasp_pcd.points.cpu(),\n",
    "                    \"vertex_colors\": grasp_pcd.colors.cpu(),  # (N, 3)\n",
    "                },\n",
    "                step=epoch,\n",
    "            )\n",
    "        \n",
    "        print(f\"(Epoch: {epoch}) Successfully saved logs to: {log_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff_edf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "79a0085b6cf04e1cff261ad12d41cff4e1530d9e68d1f8fc6bd159a2915452c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
